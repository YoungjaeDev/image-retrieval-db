{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINOv2를 사용한 이미지 분류\n",
    "\n",
    "Meta Research에서 2023년 4월에 출시한 DINOv2는 컴퓨터 비전 모델을 훈련하는 자체 지도(self-supervised method) 방법을 구현합니다.\n",
    "\n",
    "DINOv2는 레이블 없이 1억 4천만 개의 이미지를 사용하여 훈련되었습니다. DINOv2에 의해 생성된 임베딩은 분류, 이미지 검색, 세분화 및 깊이 추정에 사용할 수 있습니다. 그렇긴 하지만, Meta Research는 세분화 및 깊이 추정을 위한 헤드를 공개하지 않았습니다.\n",
    "\n",
    "이 가이드에서는 DINOv2의 임베딩을 사용하여 이미지 분류기(image classification)를 빌드할 것입니다. 이를 위해 당사는 다음을 수행합니다.\n",
    "\n",
    "1. 이미지 폴더 불러오기\n",
    "2. 각 이미지에 대한 임베딩 계산\n",
    "3. 모든 임베딩을 파일 및 벡터 저장소에 저장합니다.\n",
    "4. SVM 분류기를 훈련시켜 영상을 분류하기\n",
    "\n",
    "이 프로젝트에서는 [MIT Indoor Scene Recognition](https://universe.roboflow.com/popular-benchmarks/mit-indoor-scene-recognition/) 데이터 세트를 사용할 예정이지만, 가지고 있는 레이블이 지정된 모든 분류 데이터 세트를 사용할 수 있습니다.\n",
    "\n",
    "이 노트북이 끝날 때쯤이면 데이터 세트에 대해 훈련된 분류자를 갖게 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data 데이터 불러오기\n",
    "\n",
    "이 가이드에서는 Roboflow Universe에서 호스팅되는 MIT Indoor Scene Recognition 데이터 세트로 작업할 것입니다. 이 데이터 세트를 다운로드하려면 무료 Roboflow 계정이 필요합니다.\n",
    "\n",
    "데이터 세트를 다운로드하고 훈련 데이터 세트의 각 이미지를 연결된 레이블에 매핑하는 사전을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow supervision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roboflow\n",
    "import supervision as sv\n",
    "\n",
    "roboflow.login()\n",
    "\n",
    "rf = roboflow.Roboflow()\n",
    "\n",
    "project = rf.workspace(\"popular-benchmarks\").project(\"mit-indoor-scene-recognition\")\n",
    "dataset = project.version(5).download(\"folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# train\n",
    "ROOT_DIR = os.path.join(cwd, \"MIT-Indoor-Scene-Recognition-5/train\")\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for folder in os.listdir(ROOT_DIR):\n",
    "    for file in os.listdir(os.path.join(ROOT_DIR, folder)):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            full_name = os.path.join(ROOT_DIR, folder, file)\n",
    "            labels[full_name] = folder\n",
    "\n",
    "files = labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 불러오기 및 계산하기 임베딩(Embeddings)\n",
    "\n",
    "분류기를 훈련시키려면 다음이 필요합니다.\n",
    "\n",
    "1. 데이터 세트의 각 이미지와 연결된 임베딩\n",
    "2. 각 이미지와 연결된 레이블입니다\n",
    "\n",
    "임베딩을 계산하기 위해 DINOv2를 사용합니다. 아래에서는 가장 작은 DINOv2 가중치를 로드하고 지정된 목록의 모든 이미지에 대해 임베딩을 로드하고 계산하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/dinov2#pretrained-backbones-via-pytorch-hub\n",
    "\n",
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "dinov2_vits14.to(device)\n",
    "\n",
    "transform_image = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Resize(364), \n",
    "    T.CenterCrop(364), \n",
    "    T.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img: str) -> torch.Tensor:\n",
    "    img = Image.open(img)\n",
    "    \n",
    "    t_img = transform_image(img)[:3].unsqueeze(0)\n",
    "    \n",
    "    return t_img\n",
    "\n",
    "def compute_embedding(files: list, output_name=\"all_embeddings.json\") -> dict:\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, file in enumerate(tqdm(files)):\n",
    "            img = load_image(file).to(device)\n",
    "            \n",
    "            embeddings = dinov2_vits14(img)\n",
    "            \n",
    "            all_embeddings[file] = np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n",
    "            \n",
    "    with open(output_name, \"w\") as f:\n",
    "        f.write(json.dumps(all_embeddings))\n",
    "        \n",
    "    return all_embeddings\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 컴퓨트 임베딩(Compute Embeddings)\n",
    "\n",
    "아래 코드는 데이터 세트의 모든 이미지에 대한 임베딩을 계산합니다. 이 단계는 MIT Indoor Scene Recognition 데이터 세트에 몇 분 정도 걸립니다. 훈련 세트에는 DINOv2를 통과해야 하는 10,000개 이상의 이미지가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embeddings = compute_embedding(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 분류 모델 훈련시키기 (Train a Classification Model)\n",
    "\n",
    "우리가 계산한 임베딩은 분류 모델에서 입력으로 사용할 수 있습니다. 이 가이드에서는 선형 분류 모델인 SVM을 사용합니다.\n",
    "\n",
    "아래에서는 우리가 계산한 모든 임베딩과 관련 레이블의 목록을 만듭니다. 그런 다음 해당 목록을 사용하여 모델을 맞춥니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(embeddings.values())[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "\n",
    "y = [labels[file] for file in files]\n",
    "\n",
    "# np.array(list(embeddings.values())[0]).shape (1, 384)\n",
    "embedding_list = list(embeddings.values())\n",
    "\n",
    "clf.fit(np.array(embedding_list).reshape(-1, 1024), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 분류 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"MIT-Indoor-Scene-Recognition-5/test/elevator/elevator_google_0053_jpg.rf.41487c3b9c1690a5de26ee0218452627.jpg\"\n",
    "\n",
    "new_image = load_image(input_file)\n",
    "\n",
    "%matplotlib inline\n",
    "sv.plot_image(image=cv2.imread(input_file), size=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = dinov2_vits14(new_image.to(device))\n",
    "\n",
    "    prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
    "\n",
    "    print()\n",
    "    print(\"Predicted class: \" + prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_data(root_dir):\n",
    "    labels = {}\n",
    "    for split in ['valid', 'test']:\n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "        for folder in os.listdir(split_dir):\n",
    "            for file in os.listdir(os.path.join(split_dir, folder)):\n",
    "                if file.endswith(\".jpg\"):\n",
    "                    full_name = os.path.join(split_dir, folder, file)\n",
    "                    labels[full_name] = folder\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_root_dir = \"MIT-Indoor-Scene-Recognition-5\"\n",
    "eval_labels = load_evaluation_data(eval_root_dir)\n",
    "eval_files = list(eval_labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing embeddings for evaluation data...\")\n",
    "eval_embeddings = compute_embedding(eval_files, output_name=\"eval_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for file in tqdm(eval_files):\n",
    "        true_label = eval_labels[file]\n",
    "        embedding = eval_embeddings[file]\n",
    "        prediction = clf.predict(np.array(embedding).reshape(1, -1))\n",
    "        \n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(prediction[0])\n",
    "\n",
    "# 평가 메트릭 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vits14 -> Accuracy: 0.4957, f1: 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
